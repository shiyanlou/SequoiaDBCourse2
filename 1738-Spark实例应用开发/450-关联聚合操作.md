---
show: step
version: 1.0 
---

## 1 课程介绍

本课程将介绍 Spark 中的 Job、Stage 和 Task 的概念以及 Hive on Spark 的关联聚合操作，并通过关联聚合操作的实验结合观察 Spark UI 监控页面帮助理解 Spark 的 Job、Stage 和 Task。

#### 1.1  实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* SequoiaDB version: 5.0
* SequoiaSQL-MySQL version: 3.4
* jdk version "1.8.0_242"
* IntelliJ IDEA Community Version: 2019.3.4
* spark version: 2.4.3

#### 1.2 知识点

* Job、Stage 和 Task

  通过第一章的学习我们已经知道了Spark 集群模式的工作原理，在主程序将任务分发给 Executor 后，具体任务还会被细分成为 Job、Stage 和 Task，三者之间的关系如下图所示。

  ![1738-450-01](https://doc.shiyanlou.com/courses/1737/1207281/0c1c28ebce819a19ff557b620baadf77-0)

  在实验中将对 Hive on Spark 提交的查询语句在 Spark UI 监控展示的信息进行解释说明。

## 2 Maven 工程介绍

* 打开 SCDD-Spark 工程

  ![1738-450-02](https://doc.shiyanlou.com/courses/1737/1207281/90fddbacd4e17dcd60b0ac849544e161-0)

* 当前实验使用到的 Maven 依赖

  ```xml
          <dependency>
              <!-- hive 的 jdbc 连接依赖 -->
              <groupId>org.apache.hive</groupId>
              <artifactId>hive-jdbc</artifactId>
              <version>1.2.1</version>
          </dependency>
  
          <dependency>
              <!-- mysql jdbc 驱动 -->
              <groupId>mysql</groupId>
              <artifactId>mysql-connector-java</artifactId>
              <version>5.1.47</version>
          </dependency>
  
          <dependency>
              <!-- 使用Script Runner执行sql脚本 -->
              <groupId>com.ibatis</groupId>
              <artifactId>ibatis2-common</artifactId>
              <version>2.1.7.597</version>
          </dependency>
  ```

* 打开当前实验所在包

  ![1738-450-03](https://doc.shiyanlou.com/courses/1737/1207281/18abd48483d2a2320fae24bf9a0b919f-0)

## 3 准备数据

#### 3.1 初始化源表

```java
        // 初始化 department 表
        System.out.println("正在初始化 department 表 ……");
        MySQLUtil.initTable("src/main/resources/sql/department.sql");
        // 初始化 salary 表
        System.out.println("正在初始化 salary 表 ……");
        MySQLUtil.initTable("src/main/resources/sql/salary.sql");
        // 初始化 employee 表结构
        System.out.println("正在初始化 employee 表 ……");
        MySQLUtil.initTable("src/main/resources/sql/employee.sql");
```

将上述代码粘贴至 `!TODO -- lesson5_statistic:step1` 标签处

![1738-450-04](https://doc.shiyanlou.com/courses/1737/1207281/2d7395a7edd330bb56331382aff2ac48-0)

#### 3.2 创建关联表

```java
        // 删除已有 department 表
        String dropDepartment = "drop table if exists department";
        // 创建 department 关联表
        String linkDepartment =
                "CREATE TABLE department " +
                        "USING org.apache.spark.sql.jdbc " +
                        "OPTIONS ( " +
                        "url 'jdbc:mysql://localhost:3306', " +
                        "dbtable \"sample.department\", " +
                        "user 'root', " +
                        "password 'root', " +
                        "driver 'com.mysql.jdbc.Driver' " +
                        ")";
        // 调用 HiveUtil 工具类执行 sql 语句
        HiveUtil.doDDL(dropDepartment);
        HiveUtil.doDDL(linkDepartment);
        // 删除已有 salary 表
        String dropSalary = "drop table if exists salary";
        // 创建 salary 关联表
        String linkSalary =
                "create table salary " +
                        "USING org.apache.spark.sql.jdbc " +
                        "options( " +
                        "url 'jdbc:mysql://localhost:3306', " +
                        "dbtable \"sample.salary\", " +
                        "user 'root', " +
                        "password 'root', " +
                        "driver 'com.mysql.jdbc.Driver' " +
                        ")";
        // 调用 HiveUtil 工具类执行 sql 语句
        HiveUtil.doDDL(dropSalary);
        HiveUtil.doDDL(linkSalary);
        // 删除已有 employee 表
        String dropEmployee = "drop table if exists employee";
        // 创建 employee 关联表
        String linkEmployee =
                "CREATE TABLE employee " +
                        "USING org.apache.spark.sql.jdbc " +
                        "OPTIONS ( " +
                        "url 'jdbc:mysql://localhost:3306', " +
                        "dbtable \"sample.employee\", " +
                        "user 'root', " +
                        "password 'root', " +
                        "driver 'com.mysql.jdbc.Driver' " +
                        ")";
        // 调用 HiveUtil 工具类执行 sql 语句
        HiveUtil.doDDL(dropEmployee);
        HiveUtil.doDDL(linkEmployee);
```

将上述代码粘贴至 `!TODO -- lesson5_statistic:step2` 标签处

![1738-450-05](https://doc.shiyanlou.com/courses/1737/1207281/fb4c4f8f4ef06bba5401ce75725d740b-0)

## 4 关联查询

查询各个部门以及职位的工资情况：

```java
        // 创建 Spark 查询语句做两个不同数据源表的关联查询
        String crossQuery =
                "select s.id,d.department,s.position,s.salary " +
                        "from " +
                        "department d,salary s " +
                        "where s.department = d.id";
        // 调用 HiveUtil 工具类执行 sql 语句
        HiveUtil.doDQL(crossQuery);
```

将上述代码粘贴至 `!TODO -- lesson5_statistic:step3` 标签处

![1738-450-06](https://doc.shiyanlou.com/courses/1737/1207281/83210a79d559641969e9254d24dae9a7-0)

## 5 聚合查询

查看各个部门的平均工资以及职员人数：

```java
        // 删除已有 statistic 表
        String dropStatistic = "drop table if exists statistic";
        // 通过 CTAS 方式创建 statistic 关联表
        String linkStatistic =
                "create table statistic using org.apache.spark.sql.jdbc " +
                        "options( " +
                        "url 'jdbc:mysql://localhost:3306', " +
                        "dbtable \"sample.statistic\", " +
                        "user 'root', " +
                        "password 'root', " +
                        "driver 'com.mysql.jdbc.Driver' " +
                        ")as( " +
                        "select d.id,d.department,avg_table.salary_avg,avg_table.employee_num " +
                        "from( " +
                        "select s.department as department_id,avg(s.salary) as salary_avg,count(1) as employee_num " +
                        "from employee e,salary s " +
                        "where e.position = s.id " +
                        "group by s.department " +
                        ")as avg_table " +
                        "left join department d " +
                        "on avg_table.department_id = d.id)";
        // 调用 HiveUtil 工具类执行 sql 语句
        HiveUtil.doDDL(dropStatistic);
        HiveUtil.doDDL(linkStatistic);
        // 创建查询统计表语句
        String queryStatistic =
                "select * from statistic";
        // 调用 HiveUtil 方法查询统计表结果集
        HiveUtil.doDQL(queryStatistic);
```

将上述代码粘贴至 `!TODO -- lesson5_statistic:step4` 标签处

![1738-450-07](https://doc.shiyanlou.com/courses/1737/1207281/37fcd237d2e7dcc57ba2c65c7b2c1355-0)

## 6 运行程序

* 右键点击 Entry 类选择编辑主函数

  ![1738-450-08](https://doc.shiyanlou.com/courses/1737/1207281/5950b0c7a6adedaa832df963149abcc6-0)

* 编辑主函数入参为 `lesson5 statistic`

  ![1738-450-09](https://doc.shiyanlou.com/courses/1737/1207281/a02ba14f0a6bf8e8e280e1703448db1e-0)

* 右键点击 Entry 类选择 Run 运行程序

  ![1738-450-10](https://doc.shiyanlou.com/courses/1737/1207281/d71fe694ab9f02bdf2fd1d0aa0af5a5e-0)

* 查看运行结果

  各个部门及职位的工资情况如下：

  ![1738-450-11](https://doc.shiyanlou.com/courses/1737/1207281/3893936c763f385d4d47db634763880d-0)

  各个部门的职员人数以及平均工资情况如下：

  ![1738-450-12](https://doc.shiyanlou.com/courses/1737/1207281/b712113ffcce3260e26bc1e3a356345f-0)

## 7 Spark UI 监控界面

在 Spark 的 UI 监控平台查看本节提交的 SQL 任务，下文将通过 UI 中监控项的描述对 Job、Stage 和 Task 等概念进行解读。

* 浏览器打开 `localhost:8080` 查看 Spark UI 监控平台

  ![1738-450-13](https://doc.shiyanlou.com/courses/1737/1207281/d38e665575e219b5fa4785cd6f03af02-0)

* 打开当前运行的 thriftserver

  ![1738-450-14](https://doc.shiyanlou.com/courses/1737/1207281/5923adb418cd45a5351395e4a1e881fb-0)

* 可以看到所有 Jobs 的运行情况

  ![1738-450-15](https://doc.shiyanlou.com/courses/1737/1207281/aafea187ae7660e241021d384ee04783-0)

* 打开 Job （以关联查询的任务为例）

  ![1738-450-16](https://doc.shiyanlou.com/courses/1737/1207281/808e994fc5eaf9e267ec9f75538e6faa-0)

* 此时可以看到当前 Job 的 Stages

  ![1738-450-17](https://doc.shiyanlou.com/courses/1737/1207281/4f7994dfa21d7ecdc97a4140c7462717-0)

* 查看当前 Job 的 运行策略（**DAG** 即 *有向无环图*）

  ![1738-450-18](https://doc.shiyanlou.com/courses/1737/1207281/f756938b5fb8bb00fc38bbdd84109935-0)

* 查看所有 Jobs 的 Stages

  ![1738-450-19](https://doc.shiyanlou.com/courses/1737/1207281/168f879ae4fd06dde13a43c220b72346-0)

* 查看单个 Stage 执行情况

  ![1738-450-20](https://doc.shiyanlou.com/courses/1737/1207281/66370d166452666a14b1c8cbac0a2f6b-0)

* 可以看到当前 Stage 的运行策略

  ![1738-450-21](https://doc.shiyanlou.com/courses/1737/1207281/1f7a9097ffb3ec4f2c168ad0d83ac91f-0)

* 查看当前 Stage 的运行时间轴

  ![1738-450-22](https://doc.shiyanlou.com/courses/1737/1207281/1b0579d3a1c90b65591cfd920f4d1922-0)

* 向下滚动是当前 Stage 下所有 Task 的运行情况以及统计信息

  ![1738-450-23](https://doc.shiyanlou.com/courses/1737/1207281/fe9bbef5ee14adeacb0c3fc93c19c938-0)

## 8 总结

通过本课程可以了解 Spark 任务的 Job、Stage 和 Task 概念，并可以在 Hive on Spark 上对 MySQL 实例中的表进行查询与聚合操作，并在 Spark UI 监控平台对提交的 SQL 任务进行简单的解读。

