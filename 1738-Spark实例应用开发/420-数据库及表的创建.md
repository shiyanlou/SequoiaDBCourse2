---
show: step
version: 1.0 
---

## 1 课程介绍

本课程将介绍如何通过 jdbc 的方式在 Hive on Spark 中访问外部数据源。

#### 1.1实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* jdk version "1.8.0_242"
* SequoiaDB version: 5.0
* SequoiaSQL-MySQL version: 3.4
* Spark version: 2.4.3
* IntelliJ IDEA Community Version: 2019.3.4

#### 1.2 知识点

在 Hive on Spark 中支持 SQL 方式关联外部数据源，只要在 Spark 中创建了外部数据源的关联表，就可以按照 jdbc 标准通过 Spark 访问外部数据源。以本实验中使用的 MySQL 实例 employee 表为例：

![1738-420-01](https://doc.shiyanlou.com/courses/1738/1207281/275e286cc65d6f6ecdfd9ab2aad1f4da-0)

在 Spark 中创建关联表只需要指定数据源，无需定义表结构，在关联时 Spark 会根据结构化数据源自动生成表结构。绑定外部数据源时使用到的参数说明如下：

```sql
CREATE TABLE sample.employee
USING org.apache.spark.sql.jdbc      -- 使用Spark jdbc驱动创建关联表
OPTIONS (
url 'jdbc:mysql://localhost:3306',   -- jdbc url
dbtable "sample.employee",           -- 数据源的库名以及表名
user 'root',                         -- 数据源用户名 
password 'root',                     -- 数据源密码
driver 'com.mysql.jdbc.Driver'       -- 数据源的jdbc驱动
);
```

## 2 Maven 工程介绍

* 打开 SCDD-Spark 工程

  ![1738-420-02](https://doc.shiyanlou.com/courses/1738/1207281/059ccc288561863c0970932125a87f46-0)

* 当前实验使用到的 Maven 依赖

  ```xml
          <dependency>
              <!-- hive 的 jdbc 连接依赖 -->
              <groupId>org.apache.hive</groupId>
              <artifactId>hive-jdbc</artifactId>
              <version>1.2.1</version>
          </dependency>
  
          <dependency>
              <!-- mysql jdbc 驱动 -->
              <groupId>mysql</groupId>
              <artifactId>mysql-connector-java</artifactId>
              <version>5.1.47</version>
          </dependency>
  
          <dependency>
              <!-- 使用Script Runner执行sql脚本 -->
              <groupId>com.ibatis</groupId>
              <artifactId>ibatis2-common</artifactId>
              <version>2.1.7.597</version>
          </dependency>
  ```
  
* 打开当前实验所在包

  ![1738-420-03](https://doc.shiyanlou.com/courses/1738/1207281/147a71f9e9ae56a4e890ca0fc416c29c-0)

## 3 关联MySQL实例代码

> **说明**
>
> 程序中会调用到第一章中定义的 Hive jdbc 工具类方法

#### 3.1 创建关联表代码

```java
        // 初始化表
        String dropTable =
                "DROP TABLE " +
                        "IF " +
                        "\tEXISTS employee";
        // 调用HiveUtil的doDDL()方法初始化employee表
        HiveUtil.doDDL(dropTable);
        // 创建Spark表（将MySQL实例表作为外部数据源）
        String createLinkTable =
                "CREATE TABLE employee " +
                        "USING org.apache.spark.sql.jdbc " +
                        "OPTIONS ( " +
                        "url 'jdbc:mysql://localhost:3306', " +
                        "dbtable \"sample.employee\", " +
                        "user 'root', " +
                        "password 'root', " +
                        "driver 'com.mysql.jdbc.Driver' " +
                        ")";
        // 调用HiveUtil的doDDL()方法在Spark中创建MySQL实例employee表的关联外表
        HiveUtil.doDDL(createLinkTable);
```

将上述代码粘贴至 `!TODO -- lesson2_createtable:step1` 标签处

![1738-420-04](https://doc.shiyanlou.com/courses/1738/1207281/b94e65f73af2c84ee292367d40b3e347-0)

#### 3.2 打印关联表代码

```java
        // 查看Spark表结构
        String getDesc=
                "desc employee";
        // 调用HiveUtil的doDQL()方法查询表结构
        HiveUtil.doDQL(getDesc);
        // 查询Spark表数据
        String queryTable = "select id,name,sex,birth,phone,email,position,address from employee";
        // 调用HiveUtil的doDQL()方法查询Spark表数据
        HiveUtil.doDQL(queryTable);
```

将上述代码粘贴至 `!TODO -- lesson2_createtable:step2` 标签处

![1738-420-05](https://doc.shiyanlou.com/courses/1738/1207281/f38990ba14ce80334832c91b4bc0e1a6-0)

## 4 运行程序

* #### 右键点击 Entry 类，选择 Edit 类的主方法

  ![1738-420-06](https://doc.shiyanlou.com/courses/1738/1207281/7af8ac703f04b027834d52dfd363ff53-0)

* #### 配置主函数入参为 `lesson2 linktable`

  ![1738-420-07](https://doc.shiyanlou.com/courses/1738/1207281/a0700874c38de1f7eb9706132fce7a31-0)

* #### 右键点击 Entry 类选择 Run 运行程序

  ![1738-420-08](https://doc.shiyanlou.com/courses/1738/1207281/9177e1e19b242d2edf176a0a5837876f-0)

* #### 运行结果如下：

  ![1738-420-09](https://doc.shiyanlou.com/courses/1738/1207281/17cecda6a6cce26eb15ec833c774088d-0)

  可以看到表关联后 Spark 自动匹配了 MySQL 实例中表字段的类型，并且可以通过 Hive on Spark 直接查询在 2.3.2 中初始化的 MySQL 实例数据。

## 5 总结

经过本课程学习，我们可以通过 Hive on Spark 创建和 MySQL 实例关联的外表，并可以通过 jdbc 的方式访问  Hive on Spark 中的关联表。
