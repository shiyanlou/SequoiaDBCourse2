---
show: step
version: 1.0 
---

## 1 课程介绍

本课程将介绍 Spark 引擎解析 sql 的工作机制，并通过程序实例对 Hive on Spark 支持的数据操作进行简要说明。

### 1.1  实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* jdk version "1.8.0_242"
* spark version: 2.4.3
* SequoiaDB version: 5.0
* SequoiaSQL-MySQL version: 3.4
* IntelliJ IDEA Community Version: 2019.3.4

### 1.2 知识点

#### 1.2.1 Spark 计算引擎工作机制

![1738-440-01](https://doc.shiyanlou.com/courses/1738/1207281/89e0cd170e8749e2f8333566f5044e31-0)

Spark SQL 工作机制的核心为 **Catalyst Optimizer** ，它将用户程序中的 SQL/Dataset/DataFrame 经过一系列操作，最终转化为 Spark 系统中执行的 RDD。上图 Catalyst Optimizer 的解析流程解读如下：

* 将 SQL 或 DataFrame 经过词法和语法解析生成未绑定的逻辑计划，之后会使用不同的 Rule 应用到该逻辑计划上
* Analyzer 通过一定的规则和数据元数据（SessionCatalog、Hive MetaStore）解析未绑定的逻辑计划，生成绑定的逻辑计划
* Optimizer 将绑定的逻辑计划经过一系列的合并、列裁剪和过滤器下推等操作后生成优化的逻辑计划
* Planner 依照 Planning Strategies 对优化的逻辑计划进行 Transform 生成物理可执行计划。每个 Strategy 将某个逻辑算子转化成对应的物理算子，最终会变成 RDD 的具体操作。
* 执行物理计划计算 RDD

#### 1.2.2 不支持 UPDATE/DELETE

在第一章中我们已经介绍过 Hive on Spark 使用 Spark 作为计算引擎，会将提交给 Hive 的语句提交到 Spark 集群上进行运算处理。在本章 1.2.1 中提及，Spark 在处理 SQL 时会首先将 SQL 或 DataFrame 经过词法和语法解析生成未绑定的逻辑计划，但 Catalyst 的 parser 是不支持 update 和 delete 的，这在实验中可以得到验证。

## 2 Maven 工程介绍

* #### 打开 SCDD-Spark 工程

  ![1738-440-02](https://doc.shiyanlou.com/courses/1738/1207281/12cec9bfc3d0905b3ca75e003db710b9-0)

* #### 当前实验使用到的 Maven 依赖

  ```xml
          <dependency>
              <!-- hive 的 jdbc 连接依赖 -->
              <groupId>org.apache.hive</groupId>
              <artifactId>hive-jdbc</artifactId>
              <version>1.2.1</version>
          </dependency>
  
          <dependency>
              <!-- mysql jdbc 依赖 -->
              <groupId>mysql</groupId>
              <artifactId>mysql-connector-java</artifactId>
              <version>5.1.47</version>
          </dependency>
  
          <dependency>
              <!-- 使用Script Runner执行sql脚本 -->
              <groupId>com.ibatis</groupId>
              <artifactId>ibatis2-common</artifactId>
              <version>2.1.7.597</version>
          </dependency>
  ```

* #### 找到本节实验用到的类包

  ![1738-440-03](https://doc.shiyanlou.com/courses/1738/1207281/1a3a39b80ec628d0c43416ed1c8ce6c2-0)

## 3 程序代码

* #### 查询结果集

  ```java
          // 创建查询语句
          String sql = "select * from department";
          // 调用 HiveUtil 类查询结果集
          HiveUtil.doDQL(sql);
  ```

  将上述代码粘贴至 DataOperation 类的 `!TODO -- lesson4_dataoperation:step1` 标签处

  ![1738-440-04](https://doc.shiyanlou.com/courses/1738/1207281/368c507f8c91777814f7c70c4d9cbc58-0)

* #### 插入数据

  ```java
          // 创建插入语句
          String sql = "insert into department values(10,'商务部')";
          // 调用 HiveUtil 执行插入语句
          System.out.println("正在插入数据……");
          HiveUtil.doDML(sql);
          // 查询结果
          getAll();
  ```

  将上述代码粘贴至 DataOperation 类的 `!TODO -- lesson4_dataoperation:step2` 标签处

  ![1738-440-05](https://doc.shiyanlou.com/courses/1738/1207281/485a22084c54af495bdac1b84826a846-0)

* 更新数据

  ```java
          // 创建更新语句
          String sql = "update department set departmenmt = '宣传部' where id = 10";
          // 调用 HiveUtil 类执行更新语句
          System.out.println("正在更新数据……");
          HiveUtil.doDML(sql);
          // 查询结果
          getAll();
  ```

  将上述代码粘贴至 DataOperation 类的 `!TODO -- lesson4_dataoperation:step3` 标签处

  ![1738-440-06](https://doc.shiyanlou.com/courses/1738/1207281/56e7ec4a166c895e8aa28c358769787a-0)

* 删除数据

  ```java
          // 创建删除语句
          String sql="delete from department where id = 10";
          // 调用 HiveUtil 类执行删除语句
          System.out.println("正在删除数据……");
          HiveUtil.doDML(sql);
          // 查询结果
          getAll();
  ```

  将上述代码粘贴至 DataOperation 类的 `!TODO -- lesson4_dataoperation:step4` 标签处

  ![1738-440-07](https://doc.shiyanlou.com/courses/1738/1207281/f965ec583b07256017ad6546b20cec4e-0)

## 4 运行程序

* 右键点击 Entry 类选择编辑主函数

  ![1738-440-08](https://doc.shiyanlou.com/courses/1738/1207281/ca6de5729033173fe8a666fdffe1df6c-0)

* 编辑主函数参数为 `lesson4 dataoperation`

  ![1738-440-09](https://doc.shiyanlou.com/courses/1738/1207281/b4948f3d82ae1d576b1d17e3b57594e2-0)

* 右键点击 Entry 类选择 Run

  ![1738-440-10](https://doc.shiyanlou.com/courses/1738/1207281/fae167c5f54e5ab508d78b26fac3624d-0)

* 查看运行结果

  ![1738-440-11](https://doc.shiyanlou.com/courses/1738/1207281/4c7075e80ea70d9a1d07f19c049e73d0-0)

  可以看到，由于 Spark 的 Catalyst 优化器并不能解析 update 和 delete 词法，update 和 delete 操作都抛出了 *org.apache.spark.sql.catalyst.parser.ParseException* 。

## 5 总结

通过本课程，可以初步了解 Spark 计算引擎在解析并处理 Hive on Spark 提交的 SQL 时的工作机制（Catalyst Optimizer），并通过实验了解并验证了 Spark 并不支持 SQL 的 update 和 delete 操作。